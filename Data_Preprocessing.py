import os
import sys
import pandas as pd
import librosa
import librosa.display
import numpy as np
import matplotlib.pyplot as plt
import soundfile as sf
from tqdm import tqdm

import warnings
if not sys.warnoptions:
    warnings.simplefilter("ignore")
warnings.filterwarnings("ignore", category=DeprecationWarning)

#-----------------------Ravdees---------------------------------

ravdess = "audio_speech_actors_01-24/"
ravdess_directory_list = os.listdir(ravdess)

Crema = "AudioWAV/"
Tess = "TESS Toronto emotional speech set data/TESS Toronto emotional speech set data/"
Savee = "ALL/"

file_emotion = []
file_path = []
for i in ravdess_directory_list:
    actor = os.listdir(ravdess + i)
    for f in actor:
        part = f.split('.')[0].split('-')
        file_emotion.append(int(part[2]))
        file_path.append(ravdess + i + '/' + f)

emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])
path_df = pd.DataFrame(file_path, columns=['Path'])
ravdess_df = pd.concat([emotion_df, path_df], axis=1)
ravdess_df.Emotions.replace({1:'neutral', 2:'neutral', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust',8:'surprise'},inplace=True)



#---------------------------Crema DataFrame----------------------------


crema_directory_list = os.listdir(Crema)

file_emotion = []
file_path = []

for file in crema_directory_list:
    file_path.append(Crema + file)
    part=file.split('_')
    if part[2] == 'SAD':
        file_emotion.append('sad')
    elif part[2] == 'ANG':
        file_emotion.append('angry')
    elif part[2] == 'DIS':
        file_emotion.append('disgust')
    elif part[2] == 'FEA':
        file_emotion.append('fear')
    elif part[2] == 'HAP':
        file_emotion.append('happy')
    elif part[2] == 'NEU':
        file_emotion.append('neutral')
    else:
        file_emotion.append('Unknown')

emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])

path_df = pd.DataFrame(file_path, columns=['Path'])
Crema_df = pd.concat([emotion_df, path_df], axis=1)
# print(Crema_df.Emotions.value_counts())

#--------------------TESS dataset---------------------------

tess_directory_list = os.listdir(Tess)

file_emotion = []
file_path = []

for dir in tess_directory_list:
    directories = os.listdir(Tess + dir)
    for file in directories:
        part = file.split('.')[0]
        parts = part.split('_')
        # TESS files format: OAF_word_emotion.wav or YAF_word_emotion.wav
        if len(parts) >= 3:
            emotion = parts[2]
            if emotion == 'ps':
                file_emotion.append('surprise')
            else:
                file_emotion.append(emotion)
            file_path.append(Tess + dir + '/' + file)

emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])

path_df = pd.DataFrame(file_path, columns=['Path'])
Tess_df = pd.concat([emotion_df, path_df], axis=1)
# print(Tess_df.Emotions.value_counts())

#-------------------------SAVEE DATASET------------------

savee_directory_list = os.listdir(Savee)

file_emotion = []
file_path = []

for file in savee_directory_list:
    file_path.append(Savee + file)
    part = file.split('_')[1]
    ele = part[:-6]
    if ele == 'a':
        file_emotion.append('angry')
    elif ele == 'd':
        file_emotion.append('disgust')
    elif ele == 'f':
        file_emotion.append('fear')
    elif ele == 'h':
        file_emotion.append('happy')
    elif ele == 'n':
        file_emotion.append('neutral')
    elif ele == 'sa':
        file_emotion.append('sad')
    else:
        file_emotion.append('surprise')

emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])

path_df = pd.DataFrame(file_path, columns=['Path'])
Savee_df = pd.concat([emotion_df, path_df], axis=1)

data_path = pd.concat([ravdess_df, Crema_df, Tess_df, Savee_df], axis = 0)
data_path.to_csv("data_path.csv",index=False)
print(data_path.head())
print(data_path.Emotions.value_counts())

# Playing Sound - Commented out for training
# data,sr = sf.read('ALL/DC_n17.wav')
# ipd.Audio(data,rate=sr)
# playsound("ALL/DC_n17.wav")

# plt.figure(figsize=(10, 5))
# spectrogram = librosa.feature.melspectrogram(y=data, sr=sr, n_mels=128,fmax=8000)
# log_spectrogram = librosa.power_to_db(spectrogram)
# librosa.display.specshow(log_spectrogram, y_axis='mel', sr=sr, x_axis='time');
# plt.title('Mel Spectrogram ')
# plt.colorbar(format='%+2.0f dB')
# plt.show()


def noise(data):
    noise_amp = 0.035*np.random.uniform()*np.amax(data)
    data = data + noise_amp*np.random.normal(size=data.shape[0])
    return data

# STRETCH
def stretch(data,rate=0.8):
    return librosa.effects.time_stretch(data,rate=rate)
# SHIFT
def shift(data):
    shift_range = int(np.random.uniform(low=-5, high = 5)*1000)
    return np.roll(data, shift_range)
# PITCH
def pitch(data, sampling_rate, pitch_factor=0.7):
    return librosa.effects.pitch_shift(data, sr=sampling_rate,n_steps=pitch_factor)

# VISUALIZATION CODE - Commented out for training
# NORMAL AUDIO
# import librosa.display
# plt.figure(figsize=(12, 5))
# librosa.display.waveshow(y=data, sr=sr)
# sd.play(data,sr)
# plt.show()

# AUDIO WITH NOISE
# x = noise(data)
# plt.figure(figsize=(12,5))
# librosa.display.waveshow(y=x, sr=sr)
# sd.play(data,sr)
# plt.show()

# STRETCHED AUDIO
# x = stretch(data)
# plt.figure(figsize=(12, 5))
# librosa.display.waveshow(y=x, sr=sr)
# sd.play(data,sr)
# plt.show()

# SHIFTED AUDIO
# x = shift(data)
# plt.figure(figsize=(12,5))
# librosa.display.waveshow(y=x, sr=sr)
# sd.play(data,sr)
# plt.show()


# PITCHED AUDIO
# x = pitch(data, sr)
# plt.figure(figsize=(12, 5))
# librosa.display.waveshow(y=x, sr=sr)
# sd.play(data,sr)
# plt.show()


def zcr(data, frame_length, hop_length):
    zcr = librosa.feature.zero_crossing_rate(data, frame_length=frame_length, hop_length=hop_length)
    return np.squeeze(zcr)
def rmse(data, frame_length=2048, hop_length=512):
    rmse = librosa.feature.rms(y=data, frame_length=frame_length, hop_length=hop_length)
    return np.squeeze(rmse)
def mfcc(data, sr, frame_length=2048, hop_length=512, flatten: bool = True):
    mfcc = librosa.feature.mfcc(y=data, sr=sr,n_fft=frame_length, hop_length=hop_length)
    return np.squeeze(mfcc.T) if not flatten else np.ravel(mfcc.T)
def extract_features(data, sr=22050, frame_length=2048, hop_length=512):
    result = np.array([])
    result = np.hstack((result,
                        zcr(data, frame_length, hop_length),
                        rmse(data, frame_length, hop_length),
                        mfcc(data, sr, frame_length, hop_length)
                        ))
    return result


def get_features(path, duration=2.5, offset=0.6):
    data, sr = librosa.load(path, duration=duration, offset=offset)
    aud = extract_features(data)
    audio = np.array(aud)

    noised_audio = noise(data)
    aud2 = extract_features(noised_audio)
    audio = np.vstack((audio, aud2))

    pitched_audio = pitch(data, sr)
    aud3 = extract_features(pitched_audio)
    audio = np.vstack((audio, aud3))

    pitched_audio1 = pitch(data, sr)
    pitched_noised_audio = noise(pitched_audio1)
    aud4 = extract_features(pitched_noised_audio)
    audio = np.vstack((audio, aud4))

    return audio



X,Y=[],[]
for path,emotion,index in tqdm(zip(data_path.Path,data_path.Emotions,range(data_path.Path.shape[0]))):
    features=get_features(path)
    if index%500==0:
        print(f'{index} audio has been processed')
    for i in features:
        X.append(i)
        Y.append(emotion)
print('Done')
Emotions = pd.DataFrame(X)
Emotions['Emotions'] = Y
Emotions.to_csv('emotion.csv', index=False)
print(f'Feature extraction complete! emotion.csv created with {len(Emotions)} samples')
Emotions.head()
